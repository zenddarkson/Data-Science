# -*- coding: utf-8 -*-
"""Examen1_1MTR19_20196355_codigo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W7EyZ0pBiJSIc18zzlInI7K0LOMcDLcs
"""

# Armando Arturo Custodio Díaz
# 20196355
# a.custodio@pucp.edu.pe

# Importando librerias

# NUEVAS - GPT
import os

# CLASE
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from scipy.fft import fft, ifft

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix

# Conectando google drive con google colab
drive.mount('/content/drive')

"""#Integración de Datos"""

# Cargar la data SPI
path = '/content/drive/MyDrive/TemasH/EX1/data/'

# Specify the desired CSV files
desired_files_SPI = ['SPI_training_0.csv.zip', 'SPI_training_1.csv.zip', 'SPI_training_2.csv.zip', 'SPI_training_3.csv.zip']

# Initialize an empty list to store DataFrames
dfs = []

# Loop through each specified CSV file, read it, add the "case" column, and append it to the list
for csv_file in desired_files_SPI:
    file_path_SPI = os.path.join(path, csv_file)
    df = pd.read_csv(file_path_SPI)

    dfs.append(df)

# Concatenate all DataFrames into a single DataFrame
concatenated_SPI = pd.concat(dfs, ignore_index=True)

# Specify the desired CSV file
desired_file_AOI = 'AOI_training.csv.zip'

# Construct the file path
file_path_AOI = os.path.join(path, desired_file_AOI)

# Read the specified CSV file
concatenated_AOI = pd.read_csv(file_path_AOI)

#display(concatenated_SPI)

#display(concatenated_AOI)

"""#Exploratory Data Analysis

Limpieza de Datos
"""

# VOLUME
# AREA

# En este caso, escojo usar la regla de 99.7%, pues al usar la regla del 95%, se forman picos en los DensityPlot

def sigma_rule(df):

    rule = (100-95)/2

    data = concatenated_SPI[df].copy()

    # Calculating bounds for 2-sigma rule (approximately 2.5th and 97.5th percentiles)
    lower_bound = np.percentile(data, rule)
    upper_bound = np.percentile(data, 100-rule)

    # Identifying outliers
    outliers = data[(data < lower_bound) | (data > upper_bound)]

    # Replacing outliers with NaN values
    concatenated_SPI.loc[outliers.index, df] = np.nan

sigma_rule('Volume(%)')
sigma_rule('Area(%)')
sigma_rule('Height(um)')
sigma_rule('OffsetX(%)')
sigma_rule('OffsetY(%)')
sigma_rule('SizeX')
sigma_rule('SizeY')

"""Verificar la existencia de valores faltantes y manejarlos adecuadamente."""

def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()

        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)

        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})

        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")

        # Return the dataframe with missing information
        return mis_val_table_ren_columns

missing_values_table(concatenated_SPI)

missing_values_table(concatenated_AOI)

# Reemplazando valores numéricos nulos con la media
concatenated_SPI['Volume(%)']=concatenated_SPI['Volume(%)'].fillna(concatenated_SPI['Volume(%)'].mean())
concatenated_SPI['Area(%)']=concatenated_SPI['Area(%)'].fillna(concatenated_SPI['Area(%)'].mean())
concatenated_SPI['Height(um)']=concatenated_SPI['Height(um)'].fillna(concatenated_SPI['Height(um)'].mean())
concatenated_SPI['OffsetX(%)']=concatenated_SPI['OffsetX(%)'].fillna(concatenated_SPI['OffsetX(%)'].mean())
concatenated_SPI['OffsetY(%)']=concatenated_SPI['OffsetY(%)'].fillna(concatenated_SPI['OffsetY(%)'].mean())
concatenated_SPI['SizeX']=concatenated_SPI['SizeX'].fillna(concatenated_SPI['SizeX'].mean())
concatenated_SPI['SizeY']=concatenated_SPI['SizeY'].fillna(concatenated_SPI['SizeY'].mean())

# Eliminando valores nulos de AOI
concatenated_AOI = concatenated_AOI.dropna(subset=['RepairLabel'])

missing_values_table(concatenated_SPI)

missing_values_table(concatenated_AOI)

display(concatenated_SPI)

# Calculate relative frequencies

def relative_plot(df):
  relative_freq = concatenated_SPI[df].value_counts(normalize=True)

  # Plot
  plt.figure(figsize=(12, 9))
  relative_freq.plot(kind='bar')
  plt.ylabel('Relative Frequency')
  plt.title('Graph of ' + df)
  plt.xticks(rotation=90)

  plt.show()

relative_plot('PinNumber')
relative_plot('Result')
relative_plot('FigureID')

"""#Ingeniería de características (feature engineering):"""

aggs_SPI = {}
aggs_SPI['Volume(%)']=['mean','std','min','max','median']
aggs_SPI['Height(um)']=['mean','std','min','max','median']
aggs_SPI['Area(%)']=['mean','std','min','max','median']
aggs_SPI['OffsetX(%)']=['mean','std','min','max','median']
aggs_SPI['OffsetY(%)']=['mean','std','min','max','median']
aggs_SPI['SizeX']=['mean','std','min','max','median']
aggs_SPI['SizeY']=['mean','std','min','max','median']

Data_SPI_agg = concatenated_SPI.groupby(['PanelID', 'FigureID', 'ComponentID']).agg(aggs_SPI)
Data_SPI_agg.columns = Data_SPI_agg.columns.map('_'.join)
Data_SPI_agg =Data_SPI_agg.reset_index()

display(Data_SPI_agg)

aggs_AOI = {}
aggs_AOI['RepairLabel']=['first']

Data_AOI_agg = concatenated_AOI.groupby(['PanelID', 'FigureID', 'ComponentID']).agg(aggs_AOI)
Data_AOI_agg.columns = Data_AOI_agg.columns.map('_'.join)
Data_AOI_agg =Data_AOI_agg.reset_index()

display(Data_AOI_agg)

"""#Integración de Datos"""

Data_AOI_SPI_agg = Data_AOI_agg.merge(Data_SPI_agg,on=['PanelID', 'FigureID', 'ComponentID'],how='left')
display(Data_AOI_SPI_agg)

# Eliminar filas con valores nulos en la columna 'RepairLabel'
Data_AOI_SPI_agg = Data_AOI_SPI_agg.dropna(subset=['RepairLabel_first'])

# Filtrar las filas donde 'RepairLabel' no es igual a 'NotYetClassified'
Data_AOI_SPI_agg = Data_AOI_SPI_agg[Data_AOI_SPI_agg['RepairLabel_first'] != 'NotYetClassified']

display(Data_AOI_SPI_agg)

"""#Definición de variables independientes y dependientes

Seleccionar las variables independientes (X)
"""

X = Data_AOI_SPI_agg.drop(['PanelID', 'FigureID', 'ComponentID', 'RepairLabel_first'], axis=1)

Y_temp = Data_AOI_SPI_agg.copy()
#Adapto la data en Y para poderlo usar en el modelo predictivo
Y_temp['RepairLabel_first'] = Y_temp['RepairLabel_first'].map({'FalseScrap':0,'NotPossibleToRepair':1})

Y = Y_temp['RepairLabel_first'].copy()

display(X)
display(Y)

# División adecuada de los datos para entrenamiento y validación del modelo. (70% entrenamiento - 30% validación)
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.30, random_state=69)

import xgboost as xgb
from xgboost import plot_importance

model = xgb.XGBClassifier(n_estimators=1000)
model.fit(X_train,Y_train)

"""#Realización de predicciones"""

Y_test_pred = model.predict(X_test)

f1_score(Y_test,Y_test_pred)

accuracy_score(Y_test,Y_test_pred)

#Confusion Matrix

conf_matrix = confusion_matrix(Y_test,Y_test_pred)
plt.figure(figsize=(10,7))
sns.heatmap(conf_matrix, annot=True, fmt='g',cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('Actual Labels')
plt.xlabel('Predicted Labels')
plt.show()

"""#Feature Importance"""

# Usar Gain como métrica
importance = model.get_booster().get_score(importance_type='gain')

# Convertir el diccionario a Dataframe
importance_df = pd.DataFrame(importance.items(),columns = ['Feature', 'Importance'])

# Ordenar por importancia, jerarquía de importancia
importance_df = importance_df.sort_values(by='Importance',ascending=False).reset_index(drop=True)

importance_df.head(10)

plt.figure(figsize=(6,8))
plt.barh(importance_df['Feature'][0:10],importance_df['Importance'][0:10])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

plt.gca().invert_yaxis() # Highest importance at the top
plt.show()

"""#---"""