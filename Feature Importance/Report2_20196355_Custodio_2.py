# -*- coding: utf-8 -*-
"""Lab05_Parte2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iyg8UOrmwhld2nzjXaDIjeLrRy4Zsja2

#Lab05 - INTELIGENCIA ARTIFICIAL INDUSTRIAL (1MTR19)

**Nombre:** Armando Arturo Custodio Díaz

**Código:** 20196355
"""

# Importando librerias
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from scipy.fft import fft, ifft
import seaborn as sns
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import warnings

# Suppress openpyxl warning
warnings.simplefilter("ignore")

# Conectando google drive con google colab
drive.mount('/content/drive')

# Define the path to the data directory
data_path = '/content/drive/MyDrive/TemasH/Lab05/data_lab5'
csv_file_path = f'{data_path}/combined_data.csv'

# Read the CSV file into a DataFrame
combined_data = pd.read_csv(csv_file_path)

"""#Preprocesamiento de datos

Tratamiento de outliers, valores faltantes
"""

def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()

        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)

        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})

        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")

        # Return the dataframe with missing information
        return mis_val_table_ren_columns

missing_values_table(combined_data)

"""Como se mencionó en el EDA, es necesario tomar acciones para estos datos faltantes de time_signal_2, como en proporción a toda la data es de 0.7% y 0.6%, conviene eliminar toda la fila donde se encuentran los valores faltantes"""

# Drop rows with missing values in 'time_signal_1' or 'time_signal_2'
combined_data_cleaned = combined_data.dropna(subset=['time_signal_1', 'time_signal_2'])

"""Ahora, veremos como el conteo de datos es congruente para todas las filas de combined data"""

# Create a new variable for exploratory data analysis (EDA)
combined_EDA = pd.DataFrame()

# Display count, mean, std, min, 25%, 50%, 75%, max for each column individually
for column in combined_data_cleaned.columns:
    count_val = combined_data_cleaned[column].count()

    # Create a new row in the combined_EDA DataFrame
    new_row = pd.DataFrame({
        '': [column],
        'Count': [count_val],
    })

    # Append the new row to the combined_EDA DataFrame
    combined_EDA = pd.concat([combined_EDA, new_row], ignore_index=True)

# Transpose the combined_EDA DataFrame to have statistics as rows
combined_EDA = combined_EDA.T

# Set the first row as headers
combined_EDA.columns = combined_EDA.iloc[0]

# Exclude the "Column" row
combined_EDA = combined_EDA.iloc[1:]

# Drop the last two columns
combined_EDA = combined_EDA.iloc[:, :-2]

display(combined_EDA)

missing_values_table(combined_data_cleaned)

"""Se he relizado el tratamiento de valores faltantes con éxito

Por otro lado, es importante tener un seguimiento de los Outliers, para ello se divide ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2'] y clasifica de acuerdo a 'Tnumber' y 'Number_of_cycle'. A partir de ello se eliminan los valores outliers siguiendo la regla sigma del 99.7%
"""

import numpy as np

def sigma_rule(df, column, rule):
    # Copy the original column to avoid modifying the original data
    data = df[column].copy()

    # Calculating bounds for the specified sigma rule
    lower_bound = np.percentile(data, (100 - rule) / 2)
    upper_bound = np.percentile(data, 100 - (100 - rule) / 2)

    # Identifying outliers
    outliers = data[(data < lower_bound) | (data > upper_bound)]

    # Replacing outliers with NaN values in the original column
    df.loc[outliers.index, column] = np.nan

    return df

# Assuming 'combined_data_cleaned' is the DataFrame without rows with missing values in 'time_signal_1' or 'time_signal_2'
# Adjust the columns as per your requirement
columns_to_process = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# Define the sigma rule for outliers
sigma_rule_value = 99.7  # or any other desired value

# Create a new DataFrame to store the results
combined_data_outliers = combined_data_cleaned.copy()

# Iterate through unique values of 'Tnumber' and 'Number_of_cycle'
for tn in combined_data_outliers['Tnumber'].unique():
    for nc in combined_data_outliers['Number_of_cycle'].unique():
        # Create a mask for the current group
        group_mask = (combined_data_outliers['Tnumber'] == tn) & (combined_data_outliers['Number_of_cycle'] == nc)

        # Check if the group has at least one row before applying sigma_rule
        if group_mask.any():
            # Apply sigma_rule to each signal in the group
            for column in columns_to_process:
                combined_data_outliers.loc[group_mask, column] = sigma_rule(combined_data_outliers[group_mask], column, sigma_rule_value)

# Now, 'combined_data_outliers' contains the data with outliers replaced by NaN values within each group defined by 'Tnumber' and 'Number_of_cycle' for each signal

missing_values_table(combined_data_outliers)

#Se eliminan los valores outliers
new_combined_data = combined_data_outliers.dropna(subset=['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2'])

missing_values_table(new_combined_data)

"""Con esto tenemos nuestro "new_combined_data" pre-procesado, sin outliers, valores faltantes o columnas innecesarias"""

display(new_combined_data)

"""#Feature extraction & selection"""

# Assuming 'new_combined_data' is your DataFrame
# Adjust the columns as per your requirement
columns_to_aggregate = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# Define aggregate functions for each column
aggs = {}
for column in columns_to_aggregate:
    aggs[column] = ['mean', 'std', 'min', 'max', 'median']  # Add any other aggregate functions you need

agg_combined_data = new_combined_data.groupby(['Tnumber', 'Number_of_cycle']).agg(aggs)
agg_combined_data.columns = agg_combined_data.columns.map('-'.join)
agg_combined_data = agg_combined_data.reset_index()

agg_combined_data.head()

missing_values_table(agg_combined_data)

import pandas as pd
import warnings

# Suppress openpyxl warning
warnings.simplefilter("ignore")

# Define the path to the data directory
data_path = '/content/drive/MyDrive/TemasH/Lab05/data_lab5'

# Create an empty dataframe to store the combined data
crack_length_data = pd.DataFrame()

# Iterate through T1 to T5 folders
for t_number in range(1, 6):
    t_folder = f'T{t_number}'
    t_folder_path = os.path.join(data_path, t_folder)

    # Construct the expected Excel file name
    excel_filename = f'Description_{t_folder}.xlsx'
    excel_path = os.path.join(t_folder_path, excel_filename)

    # Check if the Excel file exists
    if os.path.isfile(excel_path):
        # Read the Excel file, skipping the first four rows
        excel_data = pd.read_excel(excel_path, skiprows=4, engine='openpyxl')

        # Search for "Number of cycle" and find the corresponding "Crack length (mm)" column
        number_of_cycle_col = excel_data.columns[excel_data.eq('Number of cycle').any(axis=0)].tolist()[0]
        crack_length_col = excel_data.columns[excel_data.eq('Crack length (mm)').any(axis=0)].tolist()[0]

        # Extract 'Number of cycle' and 'Crack length (mm)' data
        number_of_cycle_data = excel_data[number_of_cycle_col].iloc[1:].values
        crack_length_data_data = excel_data[crack_length_col].iloc[1:].values

        # Create the variable for the current T folder
        y_variable = pd.DataFrame({
            'Tnumber': [t_number] * len(number_of_cycle_data),
            'Number_of_cycle': number_of_cycle_data,
            'Crack_length_mm': crack_length_data_data
        })

        # Append the current T folder data to the combined data
        crack_length_data = pd.concat([crack_length_data, y_variable], ignore_index=True)

# Sort the combined data based on 'Tnumber'
crack_length_data = crack_length_data.sort_values(by='Tnumber').reset_index(drop=True)

# Print the combined data
crack_length_data.head()

# Merge with crack_length_data to the left
combined_result_left_crack = pd.merge(crack_length_data, agg_combined_data, on=['Tnumber', 'Number_of_cycle'], how='left')

# Reorder the columns
column_order = ['Tnumber', 'Number_of_cycle'] + [col for col in agg_combined_data.columns if col not in ['Tnumber', 'Number_of_cycle']] + ['Crack_length_mm']
combined_result_left_crack = combined_result_left_crack[column_order]

# Print the combined result
display(combined_result_left_crack)

"""#Feature extraction & selection"""

# Rename the columns for simplicity
combined_result_left_crack.columns = [col.replace('-', '_') for col in combined_result_left_crack.columns]

# Define features (X) and target variable (y)
features = combined_result_left_crack.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
target = combined_result_left_crack['Crack_length_mm']

# Filter the data for training and testing
X_train = features[combined_result_left_crack['Tnumber'] != 5]
y_train = target[combined_result_left_crack['Tnumber'] != 5]

X_test = features[combined_result_left_crack['Tnumber'] == 5]
y_test = target[combined_result_left_crack['Tnumber'] == 5]

"""#Iteración 1:"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

# Create an XGBoost regressor
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)

# Fit the regressor to the training set
xg_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = xg_reg.predict(X_test)

# Calculate and print the RMSE (Root Mean Squared Error)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("RMSE: %.2f" % (rmse))

import matplotlib.pyplot as plt

# Plot feature importance
xgb.plot_importance(xg_reg)
plt.rcParams['figure.figsize'] = [10, 5]
plt.show()

"""**Iteración 1, resumen:**

RMSE: 2.78
Cross-Validation RMSE: 2.41

Most important features:

*   ch2_signal_2_mean
*   ch2_signal_1_max
*   ch2_signal_2_std

#Iteración 2
"""

# Create a copy of the original DataFrame
new_combined_data_IT1 = new_combined_data.copy()

# Se añaden dos variables de transfromación
#new_combined_data_IT1['RatioCH11_CH22'] = new_combined_data_IT1['ch1_signal_1'] / new_combined_data_IT1['ch2_signal_2']
#new_combined_data_IT1['RatioCH21_CH22'] = new_combined_data_IT1['ch2_signal_1'] / new_combined_data_IT1['ch2_signal_2']

#new_combined_data_IT1['RatioCH11_CH21'] = new_combined_data_IT1['ch1_signal_1'] / new_combined_data_IT1['ch2_signal_1']
#new_combined_data_IT1['ProductCH21_CH22'] = new_combined_data_IT1['ch2_signal_1'] * new_combined_data_IT1['ch2_signal_2']

#new_combined_data_IT1['ProductCH11_CH22'] = new_combined_data_IT1['ch1_signal_1'] * new_combined_data_IT1['ch2_signal_2']
#new_combined_data_IT1['ProductCH21_CH12'] = new_combined_data_IT1['ch2_signal_1'] * new_combined_data_IT1['ch1_signal_2']

#new_combined_data_IT1['Diff_CH21_CH11'] = new_combined_data_IT1['ch2_signal_1'] * new_combined_data_IT1['ch1_signal_1']
new_combined_data_IT1['Sum_CH22_CH11'] = new_combined_data_IT1['ch2_signal_2'] + new_combined_data_IT1['ch2_signal_1']

new_combined_data_IT1['sqrSum_CH22_CH21'] = np.sqrt((new_combined_data_IT1['ch2_signal_2'] ** 2) + (new_combined_data_IT1['ch2_signal_1'] ** 2))
#new_combined_data_IT1['sqrSum_CH22_CH11'] = np.sqrt((new_combined_data_IT1['ch2_signal_2'] ** 2) + (new_combined_data_IT1['ch1_signal_1'] ** 2))


#####

#columns_to_drop = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# Drop specified columns
#new_combined_data_IT1 = new_combined_data_IT1.drop(columns=columns_to_drop)

#####

# Print the updated DataFrame
display(new_combined_data_IT1)

# Check Missing values
missing_values_table(new_combined_data_IT1)

#Drop Missing Values
#new_combined_data_IT1 = new_combined_data_IT1.dropna(subset=['RatioCH11_CH22', 'RatioCH11_CH21', 'RatioCH21_CH22'])

#Check Again
missing_values_table(new_combined_data_IT1)

#columns_to_aggregate = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2', 'RatioCH11_CH22','RatioCH21_CH22','RatioCH11_CH21','ProductCH21_CH22','ProductCH11_CH22','ProductCH21_CH12','Diff_CH21_CH11','Diff_CH22_CH11','sqrSum_CH21_CH11','sqrSum_CH22_CH11']
#columns_to_aggregate = ['Ratio1','Ratio2']
columns_to_aggregate = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2', 'Sum_CH22_CH11','sqrSum_CH22_CH21']


# Define aggregate functions for each column
aggs_IT1 = {}
for column in columns_to_aggregate:
    aggs_IT1[column] = ['mean', 'std', 'min', 'max', 'median']  # Add any other aggregate functions you need

agg_combined_data_IT1 = new_combined_data_IT1.groupby(['Tnumber', 'Number_of_cycle']).agg(aggs_IT1)
agg_combined_data_IT1.columns = agg_combined_data_IT1.columns.map('-'.join)
agg_combined_data_IT1 = agg_combined_data_IT1.reset_index()

agg_combined_data_IT1.head()

# Check Missing values
missing_values_table(agg_combined_data_IT1)

# Drop columns with NaN values
agg_combined_data_IT1 = agg_combined_data_IT1.replace([np.inf, -np.inf], np.nan).dropna(axis=1)

# Check Missing again
missing_values_table(agg_combined_data_IT1)

agg_combined_data_IT1.head()

# Merge with crack_length_data to the left
IT1_combined_result_left_crack = pd.merge(crack_length_data, agg_combined_data_IT1, on=['Tnumber', 'Number_of_cycle'], how='left')

# Reorder the columns
column_order = ['Tnumber', 'Number_of_cycle'] + [col for col in agg_combined_data_IT1.columns if col not in ['Tnumber', 'Number_of_cycle']] + ['Crack_length_mm']
IT1_combined_result_left_crack = IT1_combined_result_left_crack[column_order]

# Print the combined result
IT1_combined_result_left_crack.head()

# Rename the columns for simplicity
IT1_combined_result_left_crack.columns = [col.replace('-', '_') for col in IT1_combined_result_left_crack.columns]

# Define features (X) and target variable (y)
features = IT1_combined_result_left_crack.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
target = IT1_combined_result_left_crack['Crack_length_mm']

# Filter the data for training and testing
X_train = features[IT1_combined_result_left_crack['Tnumber'] != 5]
y_train = target[IT1_combined_result_left_crack['Tnumber'] != 5]

X_test = features[IT1_combined_result_left_crack['Tnumber'] == 5]
y_test = target[IT1_combined_result_left_crack['Tnumber'] == 5]

# Create an XGBoost regressor
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)

# Fit the regressor to the training set
xg_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = xg_reg.predict(X_test)

# Calculate and print the RMSE (Root Mean Squared Error)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("RMSE: %.2f" % (rmse))

import matplotlib.pyplot as plt

# Plot feature importance
xgb.plot_importance(xg_reg)
plt.rcParams['figure.figsize'] = [10, 5]
plt.show()

"""#Iteración 3"""

# Create a copy of the original DataFrame
new_combined_data_IT2 = new_combined_data.copy()

# Se añaden dos variables de transfromación
new_combined_data_IT2['Prod_CH11_CH12'] = new_combined_data_IT2['ch1_signal_1'] * new_combined_data_IT2['ch1_signal_2']
new_combined_data_IT2['Prod_CH22_CH11'] =  new_combined_data_IT2['ch2_signal_2'] * new_combined_data_IT2['ch1_signal_1']

#columns_to_drop = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# Drop specified columns
#new_combined_data_IT1 = new_combined_data_IT1.drop(columns=columns_to_drop)

#####

# Print the updated DataFrame
display(new_combined_data_IT2)

# Check Missing values
missing_values_table(new_combined_data_IT2)

#Drop Missing Values
new_combined_data_IT2 = new_combined_data_IT2.dropna(subset=['Prod_CH11_CH12', 'Prod_CH22_CH11'])

#Check Again
missing_values_table(new_combined_data_IT2)

#columns_to_aggregate = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2', 'RatioCH11_CH22','RatioCH21_CH22','RatioCH11_CH21','ProductCH21_CH22','ProductCH11_CH22','ProductCH21_CH12','Diff_CH21_CH11','Diff_CH22_CH11','sqrSum_CH21_CH11','sqrSum_CH22_CH11']
#columns_to_aggregate = ['Ratio1','Ratio2']
columns_to_aggregate = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2', 'Prod_CH11_CH12', 'Prod_CH22_CH11']


# Define aggregate functions for each column
aggs_IT2 = {}
for column in columns_to_aggregate:
    aggs_IT2[column] = ['mean', 'std', 'min', 'max', 'median']  # Add any other aggregate functions you need

agg_combined_data_IT2 = new_combined_data_IT2.groupby(['Tnumber', 'Number_of_cycle']).agg(aggs_IT2)
agg_combined_data_IT2.columns = agg_combined_data_IT2.columns.map('-'.join)
agg_combined_data_IT2 = agg_combined_data_IT2.reset_index()

agg_combined_data_IT2.head()

# Check Missing values
missing_values_table(agg_combined_data_IT2)

# Drop columns with NaN values
agg_combined_data_IT2 = agg_combined_data_IT2.replace([np.inf, -np.inf], np.nan).dropna(axis=1)

# Check Missing again
missing_values_table(agg_combined_data_IT2)

# Merge with crack_length_data to the left
IT2_combined_result_left_crack = pd.merge(crack_length_data, agg_combined_data_IT2, on=['Tnumber', 'Number_of_cycle'], how='left')

# Reorder the columns
column_order = ['Tnumber', 'Number_of_cycle'] + [col for col in agg_combined_data_IT2.columns if col not in ['Tnumber', 'Number_of_cycle']] + ['Crack_length_mm']
IT2_combined_result_left_crack = IT2_combined_result_left_crack[column_order]

# Print the combined result
IT2_combined_result_left_crack.head()

# Rename the columns for simplicity
IT2_combined_result_left_crack.columns = [col.replace('-', '_') for col in IT2_combined_result_left_crack.columns]

# Define features (X) and target variable (y)
features = IT2_combined_result_left_crack.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
target = IT2_combined_result_left_crack['Crack_length_mm']

# Filter the data for training and testing
X_train = features[IT2_combined_result_left_crack['Tnumber'] != 5]
y_train = target[IT2_combined_result_left_crack['Tnumber'] != 5]

X_test = features[IT2_combined_result_left_crack['Tnumber'] == 5]
y_test = target[IT2_combined_result_left_crack['Tnumber'] == 5]

# Create an XGBoost regressor
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)

# Fit the regressor to the training set
xg_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = xg_reg.predict(X_test)

# Calculate and print the RMSE (Root Mean Squared Error)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("RMSE: %.2f" % (rmse))

import matplotlib.pyplot as plt

# Plot feature importance
xgb.plot_importance(xg_reg)
plt.rcParams['figure.figsize'] = [10, 5]
plt.show()

"""Se tienen los siguientes Features como los mejores:


*   ch2_signal_2_mean
*   ch2_signal_1_max
*   ch1_signal_1_std

*   ch2_signal_2_max
*   SUM_CH22_CH11_max
*   ch1_signal_2_std

*   Prod_CH22_CH11_min
*   Prod_CH22_CH11_mean
*   sqrSum_CH22_CH21_median

# CROSS VALIDATION
"""

# Create a copy of the original DataFrame
final_data = new_combined_data.copy()

# Se añaden dos variables de transfromación

final_data['Sum_CH22_CH11'] = final_data['ch2_signal_2'] + final_data['ch2_signal_1']
final_data['sqrSum_CH22_CH21'] = np.sqrt((final_data['ch2_signal_2'] ** 2) + (final_data['ch2_signal_1'] ** 2))

final_data['Prod_CH11_CH12'] = final_data['ch1_signal_1'] * final_data['ch1_signal_2']
final_data['Prod_CH22_CH11'] =  final_data['ch2_signal_2'] * final_data['ch1_signal_1']


# Drop specified columns
#new_combined_data_IT1 = new_combined_data_IT1.drop(columns=columns_to_drop)

#####

# Print the updated DataFrame
display(final_data)

# Check Missing values
missing_values_table(final_data)

columns_to_aggregate = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2', 'Sum_CH22_CH11', 'sqrSum_CH22_CH21', 'Prod_CH11_CH12', 'Prod_CH22_CH11']


# Define aggregate functions for each column
aggs = {}
aggs['ch1_signal_1'] = ['std']
aggs['ch2_signal_1'] = ['max']
aggs['ch1_signal_2'] = ['std']
aggs['ch2_signal_2'] = ['mean','max']
aggs['Sum_CH22_CH11'] = ['max']
aggs['sqrSum_CH22_CH21'] = ['median']
aggs['Prod_CH22_CH11'] = ['mean', 'min']

agg_combined_data_FINAL = final_data.groupby(['Tnumber', 'Number_of_cycle']).agg(aggs)
agg_combined_data_FINAL.columns = agg_combined_data_FINAL.columns.map('-'.join)
agg_combined_data_FINAL = agg_combined_data_FINAL.reset_index()

agg_combined_data_FINAL.head()

# Check Missing values
missing_values_table(agg_combined_data_FINAL)

# Drop columns with NaN values
agg_combined_data_FINAL = agg_combined_data_FINAL.replace([np.inf, -np.inf], np.nan).dropna(axis=1)

# Check Missing again
missing_values_table(agg_combined_data_FINAL)

# Merge with crack_length_data to the left
combined_result_final = pd.merge(crack_length_data, agg_combined_data_FINAL, on=['Tnumber', 'Number_of_cycle'], how='left')

# Reorder the columns
column_order = ['Tnumber', 'Number_of_cycle'] + [col for col in agg_combined_data_FINAL.columns if col not in ['Tnumber', 'Number_of_cycle']] + ['Crack_length_mm']
combined_result_final = combined_result_final[column_order]

# Print the combined result
combined_result_final.head()

# Rename the columns for simplicity
IT2_combined_result_left_crack.columns = [col.replace('-', '_') for col in IT2_combined_result_left_crack.columns]

# Define features (X) and target variable (y)
features = IT2_combined_result_left_crack.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
target = IT2_combined_result_left_crack['Crack_length_mm']

# Filter the data for training and testing
X_train = features[IT2_combined_result_left_crack['Tnumber'] != 5]
y_train = target[IT2_combined_result_left_crack['Tnumber'] != 5]

X_test = features[IT2_combined_result_left_crack['Tnumber'] == 5]
y_test = target[IT2_combined_result_left_crack['Tnumber'] == 5]

# Create an XGBoost regressor
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)

# Fit the regressor to the training set
xg_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = xg_reg.predict(X_test)

# Calculate and print the RMSE (Root Mean Squared Error)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("RMSE: %.2f" % (rmse))

import matplotlib.pyplot as plt

# Plot feature importance
xgb.plot_importance(xg_reg)
plt.rcParams['figure.figsize'] = [10, 5]
plt.show()

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import numpy as np

# Assuming 'combined_result_left_crack' is your DataFrame
# Replace 'combined_result_left_crack' with your actual data
# Rename the columns for simplicity
combined_result_left_crack.columns = [col.replace('-', '_') for col in combined_result_left_crack.columns]

# Define features (X) and target variable (y)
features = combined_result_left_crack.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
target = combined_result_left_crack['Crack_length_mm']

# Define the number of splits for cross-validation
num_splits = 5

# Initialize an array to store RMSE scores for each iteration
rmse_scores = []

# Perform cross-validation
for i in range(1, 6):
    # Testing data for the current iteration (Ti)
    testing_data = combined_result_left_crack[combined_result_left_crack['Tnumber'] == i]

    # Training data for the current iteration (not Ti)
    training_data = combined_result_left_crack[combined_result_left_crack['Tnumber'] != i]

    # Define features and target for testing and training
    X_test = testing_data.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
    y_test = testing_data['Crack_length_mm']

    X_train = training_data.drop(['Tnumber', 'Number_of_cycle', 'Crack_length_mm'], axis=1)
    y_train = training_data['Crack_length_mm']

    # Create an XGBoost regressor
    xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                              max_depth=5, alpha=10, n_estimators=10)

    # Fit the regressor to the training set
    xg_reg.fit(X_train, y_train)

    # Predict on the test set
    y_pred = xg_reg.predict(X_test)

    # Calculate RMSE score for the current iteration
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    rmse_scores.append(rmse)

    # Display RMSE score for the current iteration
    print(f'Iteration {i}: Testing Ti = {i} | RMSE = {rmse:.2f}')

# Calculate and display the average RMSE score
average_rmse = np.mean(rmse_scores)
print(f'\nAverage RMSE across all iterations: {average_rmse:.2f}')

"""#Conclusiones

El proceso de validación cruzada se llevó a cabo para evaluar el rendimiento del modelo en diferentes conjuntos de prueba (Ti). Los resultados muestran variabilidad en el RMSE para cada iteración, indicando que el desempeño puede depender de la selección específica del conjunto de prueba. La medición promedio del RMSE a lo largo de todas las iteraciones proporciona una evaluación general del rendimiento del modelo en el conjunto de datos completo. Teninedo un menor RMSE para Testing en T2, donde se dió el mejor rendimiento de RMSE de 2.2

El modelo XGBoost mostró un buen rendimiento al predecir la longitud de las grietas en base a las señales de los instrumentos

Durante este proyecto, se ha logrado una carga y procesamiento eficiente de datos provenientes de múltiples archivos CSV y Excel, específicamente relacionados con diferentes juntas (T1 a T5). El enfoque ha sido optimizar el código para realizar cambios rápidos y automáticos, minimizando la intervención manual.

En el proceso de feature engineering, se llevaron a cabo varias iteraciones para encontrar transformaciones que destacaran en el top 3, reconociendo la complejidad de automatizar completamente esta etapa, pero buscando modularidad en el código para facilitar ajustes.

El pre-procesamiento de datos, incluida la detección y manejo de valores faltantes, infinitos y nulos, ha sido crítico para asegurar que el entrenamiento del modelo se realice de manera correcta y confiable.

#**Armando** **Arturo** **Custodio** **Díaz** **-** **20196355**
"""