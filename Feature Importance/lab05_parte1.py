# -*- coding: utf-8 -*-
"""Lab05_Parte1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D08NxbxhqpcIv4MrGFStZVrFNBu7tBdf

#Lab05 - INTELIGENCIA ARTIFICIAL INDUSTRIAL (1MTR19)

**Nombre:** Armando Arturo Custodio Díaz

**Código:** 20196355
"""

# Importando librerias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from scipy.fft import fft, ifft
import seaborn as sns
from sklearn.metrics import mean_squared_error
import xgboost as xgb

#GPT
import statsmodels.api as sm

# Conectando google drive con google colab
drive.mount('/content/drive')

"""Integración de datos preliminar de todos los datos, el código irá por cada carpeta desde T1 a T5 e irá por cada subcarpeta y orgnizará toda la data en: "combined_data"
"""

import os
import pandas as pd

# Define the path to the data directory
data_path = '/content/drive/MyDrive/TemasH/Lab05/data_lab5'

# Create an empty DataFrame to store the combined data
combined_data = pd.DataFrame()

# Iterate through T1 to T5 folders
for t_number in range(1, 6):
    t_folder = f'T{t_number}'
    t_folder_path = os.path.join(data_path, t_folder)

    # Iterate through subfolders inside each T folder
    for subfolder in os.listdir(t_folder_path):
        subfolder_path = os.path.join(t_folder_path, subfolder)

        # Check if the subfolder is a directory
        if os.path.isdir(subfolder_path):
            # Read signal_1.csv and signal_2.csv for each subfolder
            signal_1_path = os.path.join(subfolder_path, 'signal_1.csv')
            signal_2_path = os.path.join(subfolder_path, 'signal_2.csv')

            signal_1_df = pd.read_csv(signal_1_path)
            signal_2_df = pd.read_csv(signal_2_path)

            # Extract cycle number and T number from folder names
            cycle_number = int(subfolder)
            t_number = int(t_number)

            # Create new variable with the required information
            new_variable = pd.DataFrame({
                'time_signal_1': signal_1_df['time'],
                'ch1_signal_1': signal_1_df['ch1'],
                'ch2_signal_1': signal_1_df['ch2'],
                'time_signal_2': signal_2_df['time'],
                'ch1_signal_2': signal_2_df['ch1'],
                'ch2_signal_2': signal_2_df['ch2'],
                'Number_of_cycle': cycle_number,
                'Tnumber': t_number
            })

            # Append the new variable to the combined data
            combined_data = pd.concat([combined_data, new_variable], ignore_index=True)

combined_data.head()

display(combined_data)

"""#Análisis Exploratorio de Datos

Se realizará el gráfico de regresión para las señales ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2, unas con otras,1211211111111122221231231312realizará en un inicio solo par T1, luego se extrapolará a todas las carpetas
"""

# Selecting columns for regression plots
columns_to_plot = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# Creating a 4x4 grid of subplots
fig, axes = plt.subplots(4, 4, figsize=(16, 16))

# Loop through each combination of variables and create regression plots
for i, column1 in enumerate(columns_to_plot):
    for j, column2 in enumerate(columns_to_plot):
        #if i != j:
            sns.regplot(x=column1, y=column2, data=combined_data, ax=axes[i, j],
                        scatter_kws={'s': 10})  # Change the 's' parameter to adjust point size
            axes[i, j].set_title(f'{column1} and {column2}')

# Adjust layout
plt.tight_layout()
plt.show()

"""Como se puede observar, existe una regresión muy fuerte entre ch1_signal_2 y ch1_signal_1, se resumirá es información mediante una matriz de correlación"""

# Loop through each combination of variables and create regression plots
for i, column1 in enumerate(columns_to_plot):
    for j, column2 in enumerate(columns_to_plot):
        if i == j:
            # Plot diagonal histograms instead of scatter plots for self-correlation
            sns.histplot(combined_data[column1], kde=True, ax=axes[i, j])
            axes[i, j].set_title(f'{column1}')
        else:
            # Plot regression plots for non-diagonal elements
            sns.regplot(x=column1, y=column2, data=combined_data, ax=axes[i, j],
                        scatter_kws={'s': 10})  # Change the 's' parameter to adjust point size
            axes[i, j].set_title(f'{column1} and {column2}')

# Adjust layout
plt.tight_layout()
plt.show()

# Create a correlation matrix
correlation_matrix = combined_data[columns_to_plot].corr()

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""A partir de esto, podemos observar una fuerte correlación entre variables como se había mencionado, pero esto es solo para el caso de T1, a continuación se extrapolará a T1 hasta T5 y se seleccionará las correlaciones positivas o negativas con un threshold de 0.9 y -0.9"""

# Selecting columns for regression
columns_to_plot = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# List of T folders
t_folders = ['T1', 'T2', 'T3', 'T4', 'T5']

# Create an empty list to store DataFrames for each T folder
filtered_data_list = []

# Iterate through each T folder and calculate regression coefficients
for t_folder in t_folders:
    # Filter data for the specific T folder
    t_data = combined_data[combined_data['Tnumber'] == int(t_folder[1])]

    # Iterate through each combination of variables
    for column1 in columns_to_plot:
        for column2 in columns_to_plot:
            if column1 != column2:
                X = sm.add_constant(t_data[column1])  # Add a constant term to the predictor
                model = sm.OLS(t_data[column2], X).fit()  # Fit the regression model
                coefficient = model.params[1]

                # Check if the coefficient meets the filtering criteria
                if coefficient > 0.9 or coefficient < -0.9:
                    filtered_data_list.append(pd.DataFrame({
                        'Tnumber': [t_folder],
                        'Variable1': [column1],
                        'Variable2': [column2],
                        'Coefficient': [coefficient]
                    }))

# Concatenate the list of DataFrames into a single DataFrame
filtered_data = pd.concat(filtered_data_list, ignore_index=True)

# Display the filtered data
print(filtered_data)

"""Luego de analizar estas correlaciones, por ejemplo:


*   ch1_signal_1  ch1_signal_2 para T2
*   ch1_signal_2  ch1_signal_1 para T4

Se puede concluir que esta redundancia de datos puede ser tratada para disminuir la capacidad computacional que tendrá el modelo y así obviar datos que ya están siendo explicados y cuya correlación es mayor a 0.9, incluso algunos llegan a valores mayores a 0.98

Finalmente, para un mayor entendimiento de esta data, se ha organizado un gráfico para poder visualizar esta correlación entre todos los valores 'ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2' para todos los caosos 'T1', 'T2', 'T3', 'T4', 'T5'
"""

# Selecting columns for regression
columns_to_plot = ['ch1_signal_1', 'ch2_signal_1', 'ch1_signal_2', 'ch2_signal_2']

# List of T folders
t_folders = ['T1', 'T2', 'T3', 'T4', 'T5']

# Create an empty DataFrame to store regression coefficients
coefficients_df = pd.DataFrame(index=columns_to_plot, columns=pd.MultiIndex.from_product([columns_to_plot, t_folders]))

# Iterate through each T folder and calculate regression coefficients
for t_folder in t_folders:
    # Filter data for the specific T folder
    t_data = combined_data[combined_data['Tnumber'] == int(t_folder[1])]

    # Iterate through each combination of variables
    for column1 in columns_to_plot:
        for column2 in columns_to_plot:
            if column1 != column2:
                X = sm.add_constant(t_data[column1])  # Add a constant term to the predictor
                model = sm.OLS(t_data[column2], X).fit()  # Fit the regression model
                coefficients_df.loc[column1, (column2, t_folder)] = model.params[1]

# Create a grouped bar plot with different colors for each T folder and a larger size
plt.figure(figsize=(20, 12))  # Increase the values here to make the figure larger
colors = sns.color_palette('husl', n_colors=len(t_folders))
coefficients_df.plot(kind='bar', color=colors, width=0.8)
plt.title('Regression Coefficients for Different T Folders')
plt.xlabel('Variables')
plt.ylabel('Regression Coefficient')
plt.legend(title='T Folders', bbox_to_anchor=(1.05, 1), loc='upper left')
# Set y-axis limits
plt.ylim(-0.5, 1.05)
plt.show()

"""Por otro lado, se resume los datos obtenidos en combined_data de la siguiente manera"""

# Create a new variable for exploratory data analysis (EDA)
combined_EDA = pd.DataFrame()

# Display count, mean, std, min, 25%, 50%, 75%, max for each column individually
for column in combined_data.columns:
    count_val = combined_data[column].count()
    mean_val = combined_data[column].mean()
    std_val = combined_data[column].std()
    min_val = combined_data[column].min()
    percentile_25 = combined_data[column].quantile(0.25)
    median_val = combined_data[column].median()
    percentile_75 = combined_data[column].quantile(0.75)
    max_val = combined_data[column].max()

    # Create a new row in the combined_EDA DataFrame
    new_row = pd.DataFrame({
        '': [column],
        'Count': [count_val],
        'Mean': [mean_val],
        'Std': [std_val],
        '25% Percentile': [percentile_25],
        '50% Percentile (Median)': [median_val],
        '75% Percentile': [percentile_75],
        'Min': [min_val],
        'Max': [max_val]
    })

    # Append the new row to the combined_EDA DataFrame
    combined_EDA = pd.concat([combined_EDA, new_row], ignore_index=True)

# Transpose the combined_EDA DataFrame to have statistics as rows
combined_EDA = combined_EDA.T

# Set the first row as headers
combined_EDA.columns = combined_EDA.iloc[0]

# Exclude the "Column" row
combined_EDA = combined_EDA.iloc[1:]

# Drop the last two columns
combined_EDA = combined_EDA.iloc[:, :-2]

display(combined_EDA)

"""Como podemos observar, las señales de tiempo tienen missing values, que deben ser tratados posteriormente. Además, se puede considerar la existencia de outliers, en el caso de ch1_signal_1 y ch1_signal_2 en sus valores máximos y mínimos

Por otro lado, se tiene el análsis de señales en el tiempo y esto se ha automatizado mediante la función "plot_signal_by_cycle", a la cual podemos darle como input la señal que deseamos, su valor time correspondiente y el t_number que analiza cada ciclo (subcarpeta) dentro cada Tx (x = 1,2,3,4,5). Es importante tener en cuenta que para este gráfico, dropna(subset=[time, signal]) se encarga de eliminar las filas que contienen valores faltantes en las columnas especificadas, que son time y signal. Esto asegura que solo se incluyan las filas que tienen valores válidos tanto para el tiempo como para la señal que estás analizando en esa subcarpeta específica.
"""

def plot_signal_by_cycle(data, signal, time, t_number):
    # Filter data for the specified Tnumber
    t_data = data[(data['Tnumber'] == t_number)]

    # Get unique subfolders (Number_of_cycle)
    subfolders = t_data['Number_of_cycle'].unique()

    # Create subplots for each subfolder
    fig, axes = plt.subplots(nrows=len(subfolders), ncols=1, figsize=(10, 3 * len(subfolders)), sharex=True)

    # Iterate through subfolders
    for subfolder, ax in zip(subfolders, axes):
        # Filter data for the current subfolder and remove rows with NaN values
        subset_data = t_data[t_data['Number_of_cycle'] == subfolder].dropna(subset=[time, signal])

        # Plot the data for the specified signal against time
        ax.plot(subset_data[time], subset_data[signal], label=f'Subfolder {subfolder}')

        # Set labels and title
        ax.set_ylabel(signal)
        ax.set_title(f'T{t_number} - Subfolder {subfolder}')

        # Add time values to the X-axis
        ax.set_xlabel('Time')

    # Add common x-axis label
    plt.xlabel('Time')

    # Add legend
    plt.legend()

    # Adjust layout
    plt.tight_layout()

    # Show the plot
    plt.show()

"""En primera instancia se analizará la señal ch1_signal_1, time_signal_1, en la carpeta T3"""

# Call the function with your specific parameters
plot_signal_by_cycle(combined_data, signal='ch1_signal_1', time='time_signal_1', t_number=3)

"""En segunda instancia se analizará la señal ch1_signal_1, time_signal_1, en la carpeta T1"""

# Call the function with your specific parameters
plot_signal_by_cycle(combined_data, signal='ch1_signal_1', time='time_signal_1', t_number=1)

"""Se analizará la señal ch2_signal_1, time_signal_1, en la carpeta T1"""

# Call the function with your specific parameters
plot_signal_by_cycle(combined_data, signal='ch2_signal_1', time='time_signal_1', t_number=1)

"""Se analizará la señal ch2_signal_2, time_signal_2, en la carpeta T4"""

# Call the function with your specific parameters
plot_signal_by_cycle(combined_data, signal='ch2_signal_2', time='time_signal_2', t_number=5)

"""De esto se puede concluir que los datos obtenidos desde la señal ch2 tiene mucha más variaciones que en el caso de ch1, y esto muestra una diferencia significativa, pues en el caso de ch1, se obser una concentración

Finalmente, se analizan los Density Plot, en un código automatizado, donde se puede evalar la señal y la carpeta dependiendo de los ciclos en la función "plot_density_by_cycle"
"""

def plot_density_by_cycle(data, signal, t_number):
    # Filter data for the specified Tnumber
    t_data = data[(data['Tnumber'] == t_number)]

    # Get unique subfolders (Number_of_cycle)
    subfolders = t_data['Number_of_cycle'].unique()

    # Create subplots for each subfolder
    fig, axes = plt.subplots(nrows=len(subfolders), ncols=1, figsize=(10, 3 * len(subfolders)))

    # Iterate through subfolders
    for subfolder, ax in zip(subfolders, axes):
        # Filter data for the current subfolder and remove rows with NaN values
        subset_data = t_data[t_data['Number_of_cycle'] == subfolder].dropna(subset=[signal])

        # Create density plot for the specified signal
        sns.kdeplot(subset_data[signal], ax=ax, label=f'Subfolder {subfolder}', fill=True)

        # Set labels and title
        ax.set_ylabel('Density')
        ax.set_title(f'{signal} Density - T{t_number} - Subfolder {subfolder}')

    # Add legend
    plt.legend()

    # Adjust layout
    plt.tight_layout()

    # Show the plot
    plt.show()

# Call the function with your specific parameters
plot_density_by_cycle(combined_data, signal='ch1_signal_1', t_number=3)

# Call the function with your specific parameters
plot_density_by_cycle(combined_data, signal='ch2_signal_2', t_number=3)

# Call the function with your specific parameters
plot_density_by_cycle(combined_data, signal='ch1_signal_2', t_number=5)

# Call the function with your specific parameters
plot_density_by_cycle(combined_data, signal='ch2_signal_1', t_number=4)

"""Dado esto, se puede confirmar las afirmaciones hechas sobre ch1 y ch2 analizadas en el dominio del tiempo. Desde el Density Plot, podemos concluir además que la señal ch2 tiene una distrubución entre un rango de valores mucho más extenso que para ch1, todo tiende a concentrarse en un valor"""

# Guardar Combined Data:
csv_file_path = f'{data_path}/combined_data.csv'

# Save the DataFrame to a CSV file
combined_data.to_csv(csv_file_path, index=False)

print(f'Data saved to: {csv_file_path}')